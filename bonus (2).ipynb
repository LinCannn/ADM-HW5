{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connected Components on MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this section is to identify the connected components in the graph, while filtering the data based on a specific time interval. It focuses on identifying the connected components, calculating the total number of flights (edges) during that period, and determining the unique airports (nodes) involved.\n",
        "\n",
        "Additionally, We provide the number of connected components identified, along with their sizes. We also identify the airports belonging to the largest component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Union-Find algorithm was used, and it is based on two main operations:\n",
        "\n",
        "- **Find:** This operation helps figure out the representative of the set that a particular node belongs to. If two nodes have the same representative, it means they're part of the same connected component. \n",
        "\n",
        "```python\n",
        "       def find(node):\n",
        "           if node not in components:\n",
        "               components[node] = node\n",
        "           if components[node] != node:\n",
        "               components[node] = find(components[node])\n",
        "           return components[node]\n",
        "```\n",
        "\n",
        "- **Union**This operation connects two nodes and their respective components. If the nodes belong to different components, the operation joins them into a new component\n",
        "       \n",
        "                                  \n",
        "```python\n",
        "def union(node1, node2):\n",
        "    root1, root2 = find(node1), find(node2)  # Trova i rappresentanti dei due nodi\n",
        "    if root1 != root2:  # Se sono in componenti diverse\n",
        "        components[root2] = root1  # Unisce le componenti\n",
        "```\n",
        "The algorithm stops when it reaches convergence, meaning when the component mapping (prev_components) matches the one obtained in the current iteration (curr_components). This indicates that all connections between nodes have been propagated, and no further changes need to be made.\n",
        "In the following section, we introduce the implementation using GraphFrame that utilizes predefined algorithms (such as Connected Components)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJHnkXWpU9rZ",
        "outputId": "6ff4e0f0-98b3-4fd1-8ee8-c39cc1615859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing flight network from 1990-01-01 to 2005-12-31\n",
            "Maximum iterations set to: 10\n",
            "Loading and filtering flight network data...\n",
            "Total flights in the period: 2699030\n",
            "Unique airports: 663\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Converged at iteration 5\n"
          ]
        }
      ],
      "source": [
        "from auxiliary_files.ConnectedComponents import *\n",
        "csv = '/content/drive/MyDrive/ADM-HW5/archive_usa_airport/flights_final.csv'\n",
        "\n",
        "# DATES must be in the format YYYY-MM-DD between 1990 and 2009\n",
        "start_date = '1990-01-01'\n",
        "end_date = '2005-12-31'\n",
        "\n",
        "connections = find_connected_connections_spark(csv, start_date, end_date)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ7sAJoKXEAZ",
        "outputId": "2044493c-360d-4e14-c0bd-5fb7d2bb4ddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'number_of_components': 1,\n",
              " 'component_sizes': [663],\n",
              " 'largest_component_size': 663}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_M4HlAkHcH"
      },
      "source": [
        "## GraphFrames implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxnFizxLcE1G"
      },
      "outputs": [],
      "source": [
        "from graphframes import GraphFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7RiGEnR-XZ_l"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from graphframes import GraphFrame\n",
        "import time\n",
        "\n",
        "def find_connected_connections_graph(csv_path: str, start_date: str, end_date: str) -> dict:\n",
        "\n",
        "\n",
        "    # Initialize the Spark session\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"GraphFramesExample\") \\\n",
        "        .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Set checkpoint directory for GraphFrames\n",
        "    spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
        "\n",
        "    print(f\"Analyzing flight network from {start_date} to {end_date}\")\n",
        "\n",
        "    # Load and filter flight network data\n",
        "    print(\"Loading and filtering flight network data...\")\n",
        "    df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "\n",
        "    # Convert \"Fly_date\" column to date and apply the date range filter\n",
        "    df = df.withColumn(\"Fly_date\", col(\"Fly_date\").cast(\"date\"))\n",
        "    df = df.filter((col(\"Fly_date\") >= start_date) & (col(\"Fly_date\") <= end_date))\n",
        "    df = df.select(\"Origin_airport\", \"Destination_airport\", \"Fly_date\")\n",
        "\n",
        "    # Create a DataFrame of unique airports (vertices)\n",
        "    unique_airports = df.select(\"Origin_airport\").distinct().union(df.select(\"Destination_airport\").distinct()).distinct()\n",
        "    vertices = unique_airports.withColumnRenamed(\"Origin_airport\", \"id\")\n",
        "\n",
        "    # Create edges DataFrame with source (src) and destination (dst)\n",
        "    edges = df.withColumnRenamed(\"Origin_airport\", \"src\").withColumnRenamed(\"Destination_airport\", \"dst\")\n",
        "\n",
        "    # Create GraphFrame\n",
        "    g = GraphFrame(vertices, edges)\n",
        "\n",
        "    # Finding connected components\n",
        "    print(\"Finding connected components...\")\n",
        "    result = g.connectedComponents()\n",
        "\n",
        "    # Calculate component sizes\n",
        "    component_sizes = result.groupBy(\"component\").count().collect()\n",
        "    sizes = [row[\"count\"] for row in component_sizes]\n",
        "\n",
        "    # Find the largest component\n",
        "    largest_component_id = max(component_sizes, key=lambda x: x[\"count\"])[\"component\"]\n",
        "    largest_component = result.filter(result.component == largest_component_id).select(\"id\").rdd.map(lambda x: x[0]).collect()\n",
        "\n",
        "\n",
        "    final_result = {\n",
        "        'number_of_components': len(component_sizes),\n",
        "        'component_sizes': sizes,\n",
        "        'largest_component_size': max(sizes),\n",
        "\n",
        "    }\n",
        "\n",
        "    # Stop the Spark session\n",
        "    spark.stop()\n",
        "\n",
        "    return final_result\n",
        "\n",
        "def process_in_parallel(csv_path: str, start_date: str, end_date: str, num_partitions: int = None):\n",
        "    \"\"\"\n",
        "    Wrapper function to handle the parallel processing with configurable partitions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the Spark session and configure it (no need for SparkContext directly)\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"ParallelConnectedComponents\") \\\n",
        "            .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        if num_partitions:\n",
        "            spark.conf.set(\"spark.default.parallelism\", str(num_partitions))\n",
        "\n",
        "        # Run the analysis using the graph-based method\n",
        "        result = find_connected_connections_graph(csv_path, start_date, end_date)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in parallel processing: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Stop the Spark session\n",
        "        spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhoZE3cgXdwn",
        "outputId": "05ab10d4-afd8-4fcf-8dec-b2726768a5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing flight network from 1990-01-01 to 2005-12-31\n",
            "Loading and filtering flight network data...\n",
            "Finding connected components...\n"
          ]
        }
      ],
      "source": [
        "from auxiliary_files.ConnectedComponents import *\n",
        "csv = '/content/drive/MyDrive/ADM-HW5/archive_usa_airport/flights_final.csv'\n",
        "\n",
        "start_date = '1990-01-01'\n",
        "end_date = '2005-12-31'\n",
        "connections_GraphFrame = find_connected_connections_graph(csv, start_date, end_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Fk5iwlXdtC",
        "outputId": "79210a86-774a-4415-9810-03f80fa7dca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'number_of_components': 1,\n",
              " 'component_sizes': [663],\n",
              " 'largest_component_size': 663}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "connections_GraphFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Time comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The temporal analysis highlighted significant performance differences between the RDD-based implementation and the GraphFrames implementation for identifying connected components in a flight graph. The RDD approach proved to be considerably faster, with a total time of  91.73 seconds compared to 316.58 seconds for GraphFrames, showing an advantage of 224.85 seconds\n",
        "\n",
        "This result seems to be justified by various factors: \n",
        "- the **`setup_time`** of RDD (without GraphFrames) was **extremely faster**, the data loading took **9.36 seconds less**\n",
        "\n",
        "- the **``processing_time`** itself with the Union-Find algorithm was more efficient algorithm used by GraphFrames, saving 85.88 seconds.Maybe this is due to the iterative nature of the **connected components** algorithm used by GraphFrames. Initially, each vertex is assigned a unique component ID.Each vertex must repeatedly propagate its component ID to neighbors, updating to the lowest ID found. This process may require many iterations to achieve convergence. \n",
        "\n",
        "- Additionally, the **`component_analysis`** with RDD showed a significantly reduced time, with an advantage of **93.30  seconds** over GraphFrames.\n",
        "\n",
        "- Overall, the RDD implementation confirmed greater optimization.\n",
        "\n",
        "\n",
        "GraphFrames. \"GraphFrame.connectedComponents.\" GraphFrames Documentation, n.d., https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html#graphframes.GraphFrame.connectedComponents. Accessed 23 Dec. 2024."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7s_ZwHWBQde",
        "outputId": "15ec685b-c02b-435d-ed0b-a2805c50a889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Original Implementation...\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Converged at iteration 5\n",
            "\n",
            "Running GraphFrames Implementation...\n",
            "\n",
            "Timing Analysis:\n",
            "--------------------------------------------------------------------------------\n",
            "Section                   Original (s)    GraphFrames (s) Difference (s) \n",
            "--------------------------------------------------------------------------------\n",
            "setup_time                          0.00           0.44          -0.44\n",
            "data_loading_time                  27.95          18.59           9.36\n",
            "processing_time                    61.84         204.25        -142.41\n",
            "component_analysis_time             1.94          93.30         -91.36\n",
            "total_time                         91.73         316.58        -224.85\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from typing import Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "@dataclass\n",
        "class TimingMetrics:\n",
        "    setup_time: float = 0\n",
        "    data_loading_time: float = 0\n",
        "    processing_time: float = 0\n",
        "    component_analysis_time: float = 0\n",
        "    total_time: float = 0\n",
        "\n",
        "def measure_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        return result, end_time - start_time\n",
        "    return wrapper\n",
        "\n",
        "def find_connected_connections_with_timing(csv_path: str, start_date: str, end_date: str) -> Dict[str, Any]:\n",
        "    metrics = TimingMetrics()\n",
        "    total_start = time.time()\n",
        "\n",
        "    # Setup timing\n",
        "    setup_start = time.time()\n",
        "    spark = SparkSession.builder.appName(\"ConnectedComponents\").getOrCreate()\n",
        "    metrics.setup_time = time.time() - setup_start\n",
        "\n",
        "    # Data loading timing\n",
        "    loading_start = time.time()\n",
        "    df = spark.read.csv(csv_path, header=True)\n",
        "    df = df.filter((df.Fly_date >= start_date) & (df.Fly_date <= end_date))\n",
        "    df = df.select(\"Origin_airport\", \"Destination_airport\", \"Fly_date\")\n",
        "    metrics.data_loading_time = time.time() - loading_start\n",
        "\n",
        "    # Processing timing\n",
        "    processing_start = time.time()\n",
        "\n",
        "    def emit_bidirectional_edges(row):\n",
        "        origin = row.Origin_airport\n",
        "        dest = row.Destination_airport\n",
        "        return [(origin, dest), (dest, origin)]\n",
        "\n",
        "\n",
        "    edges_rdd = df.rdd.flatMap(emit_bidirectional_edges).distinct()\n",
        "\n",
        "\n",
        "    def update_components(edges):\n",
        "        components = {}\n",
        "\n",
        "        def find(node):\n",
        "            if node not in components:\n",
        "                components[node] = node\n",
        "            if components[node] != node:\n",
        "                components[node] = find(components[node])\n",
        "            return components[node]\n",
        "\n",
        "        def union(node1, node2):\n",
        "            root1, root2 = find(node1), find(node2)\n",
        "            if root1 != root2:\n",
        "                components[root2] = root1\n",
        "\n",
        "\n",
        "        for origin, dest in edges:\n",
        "            union(origin, dest)\n",
        "\n",
        "\n",
        "        return [(node, find(node)) for node in components.keys()]\n",
        "\n",
        "\n",
        "    components_rdd = edges_rdd.mapPartitions(lambda x: update_components(x))\n",
        "\n",
        "\n",
        "    prev_components = None\n",
        "    curr_components = components_rdd.collectAsMap()\n",
        "\n",
        "    iteration = 1\n",
        "    while prev_components != curr_components and iteration <= 10:\n",
        "        print(f\"Iteration {iteration}\")\n",
        "        prev_components = curr_components\n",
        "        components_rdd = edges_rdd.map(\n",
        "            lambda edge: (edge[1], curr_components.get(edge[0]))\n",
        "        ).filter(\n",
        "            lambda x: x[1] is not None\n",
        "        ).reduceByKey(\n",
        "            lambda x, y: min(x, y)\n",
        "        )\n",
        "\n",
        "        curr_components = components_rdd.collectAsMap()\n",
        "        iteration += 1\n",
        "\n",
        "\n",
        "    if prev_components == curr_components:\n",
        "        print(f\"Converged at iteration {iteration - 1}\")\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "\n",
        "\n",
        "    metrics.processing_time = time.time() - processing_start\n",
        "\n",
        "    # Component analysis timing\n",
        "    analysis_start = time.time()\n",
        "    final_components_df = spark.createDataFrame(\n",
        "        components_rdd.map(lambda x: (x[0], x[1])).collect(),\n",
        "        [\"node\", \"component_id\"]\n",
        "    )\n",
        "\n",
        "    component_sizes = final_components_df.groupBy(\"component_id\").count().collect()\n",
        "    sizes = [row[\"count\"] for row in component_sizes]\n",
        "\n",
        "    largest_component_id = max(component_sizes, key=lambda x: x[\"count\"])[\"component_id\"]\n",
        "    largest_component = final_components_df.filter(\n",
        "        final_components_df.component_id == largest_component_id\n",
        "    ).select(\"node\").rdd.map(lambda x: x[0]).collect()\n",
        "    metrics.component_analysis_time = time.time() - analysis_start\n",
        "\n",
        "    metrics.total_time = time.time() - total_start\n",
        "\n",
        "    result = {\n",
        "        'number_of_components': len(component_sizes),\n",
        "        'component_sizes': sizes,\n",
        "        'largest_component_size': max(sizes),\n",
        "        'timing_metrics': {\n",
        "            'setup_time': metrics.setup_time,\n",
        "            'data_loading_time': metrics.data_loading_time,\n",
        "            'processing_time': metrics.processing_time,\n",
        "            'component_analysis_time': metrics.component_analysis_time,\n",
        "            'total_time': metrics.total_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "    spark.stop()\n",
        "    return result\n",
        "\n",
        "def find_connected_connections_graph_with_timing(csv_path: str, start_date: str, end_date: str) -> Dict[str, Any]:\n",
        "    metrics = TimingMetrics()\n",
        "    total_start = time.time()\n",
        "\n",
        "    # Setup timing\n",
        "    setup_start = time.time()\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"GraphFramesExample\") \\\n",
        "        .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "        .getOrCreate()\n",
        "    spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
        "    metrics.setup_time = time.time() - setup_start\n",
        "\n",
        "    # Data loading timing\n",
        "    loading_start = time.time()\n",
        "    df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "    df = df.withColumn(\"Fly_date\", col(\"Fly_date\").cast(\"date\"))\n",
        "    df = df.filter((col(\"Fly_date\") >= start_date) & (col(\"Fly_date\") <= end_date))\n",
        "    df = df.select(\"Origin_airport\", \"Destination_airport\", \"Fly_date\")\n",
        "    metrics.data_loading_time = time.time() - loading_start\n",
        "\n",
        "    # Processing timing\n",
        "    processing_start = time.time()\n",
        "    vertices = df.select(\"Origin_airport\").distinct().union(\n",
        "        df.select(\"Destination_airport\").distinct()\n",
        "    ).distinct().withColumnRenamed(\"Origin_airport\", \"id\")\n",
        "\n",
        "    edges = df.withColumnRenamed(\"Origin_airport\", \"src\") \\\n",
        "              .withColumnRenamed(\"Destination_airport\", \"dst\")\n",
        "\n",
        "    g = GraphFrame(vertices, edges)\n",
        "    result = g.connectedComponents()\n",
        "    metrics.processing_time = time.time() - processing_start\n",
        "\n",
        "    # Component analysis timing\n",
        "    analysis_start = time.time()\n",
        "    component_sizes = result.groupBy(\"component\").count().collect()\n",
        "    sizes = [row[\"count\"] for row in component_sizes]\n",
        "\n",
        "    largest_component_id = max(component_sizes, key=lambda x: x[\"count\"])[\"component\"]\n",
        "    largest_component = result.filter(result.component == largest_component_id) \\\n",
        "                            .select(\"id\").rdd.map(lambda x: x[0]).collect()\n",
        "    metrics.component_analysis_time = time.time() - analysis_start\n",
        "\n",
        "    metrics.total_time = time.time() - total_start\n",
        "\n",
        "    final_result = {\n",
        "        'number_of_components': len(component_sizes),\n",
        "        'component_sizes': sizes,\n",
        "        'largest_component_size': max(sizes),\n",
        "        'timing_metrics': {\n",
        "            'setup_time': metrics.setup_time,\n",
        "            'data_loading_time': metrics.data_loading_time,\n",
        "            'processing_time': metrics.processing_time,\n",
        "            'component_analysis_time': metrics.component_analysis_time,\n",
        "            'total_time': metrics.total_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "    spark.stop()\n",
        "    return final_result\n",
        "\n",
        "def compare_implementations_with_timing(csv_path: str, start_date: str, end_date: str):\n",
        "    print(\"Running Original Implementation...\")\n",
        "    original_result = find_connected_connections_with_timing(csv_path, start_date, end_date)\n",
        "\n",
        "    print(\"\\nRunning GraphFrames Implementation...\")\n",
        "    graphframes_result = find_connected_connections_graph_with_timing(csv_path, start_date, end_date)\n",
        "\n",
        "    print(\"\\nTiming Analysis:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Section':<25} {'Original (s)':<15} {'GraphFrames (s)':<15} {'Difference (s)':<15}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    sections = ['setup_time', 'data_loading_time', 'processing_time', 'component_analysis_time', 'total_time']\n",
        "\n",
        "    for section in sections:\n",
        "        orig_time = original_result['timing_metrics'][section]\n",
        "        graph_time = graphframes_result['timing_metrics'][section]\n",
        "        diff = orig_time - graph_time\n",
        "        print(f\"{section:<25} {orig_time:>14.2f} {graph_time:>14.2f} {diff:>14.2f}\")\n",
        "\n",
        "    return original_result, graphframes_result\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/ADM-HW5/archive_usa_airport/flights_final.csv'\n",
        "start_date = '1990-01-01'\n",
        "end_date = '2005-12-31'\n",
        "\n",
        "original_result, graphframes_result = compare_implementations_with_timing(csv_path, start_date, end_date)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
